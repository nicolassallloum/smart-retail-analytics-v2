Smart Retail Analytics
End-to-End Data Engineering & Retail Analytics Project


[Docker: Containerized]  [PostgreSQL: 15]  [Python: ETL]  [Power BI: Visualization]


1.	Project Overview

Smart Retail Analytics is a complete end-to-end data engineering project designed to simulate a real-world retail data warehouse architecture. This project reflects enterprise-grade retail analytics architecture suitable for production-scale systems.
The project demonstrates the following key capabilities:
 	Data ingestion and transformation (ETL)
 	Star schema data warehouse modeling
 	PostgreSQL-based analytical storage
 	Dockerized infrastructure
 	Business Intelligence integration (Power BI)



2.	Architecture Overview



3.	Tech Stack

 
Layer	Technology
Containerization	Docker & Docker Compose
Data Warehouse	PostgreSQL 15
ETL	Python (Pandas, Psycopg2 / SQLAlchemy)
Data Modeling	Star Schema
BI Layer	Power BI Desktop
Optional NoSQL	MongoDB

4.	Project Structure



5.	Data Warehouse Design

The data warehouse follows a strict Star Schema Model to optimize for analytical queries.

Fact Table
fact_sales
 	order_id
 	date_id
 	customer_id
 	product_id
 	quantity
 	gross_amount
 	net_amount
 
 	discount
 	payment_method
 	is_returned

Dimension Tables
 	dim_customer: Customer demographics and details.
 	dim_product: Product categorization and pricing.
 	dim_date: Date hierarchy for temporal analysis.



6.	How to Run the Project

1.	Clone the Repository

2.	Start Infrastructure


PostgreSQL will run on:

3.	Run ETL Pipeline
Set environment variable:


Run the loader script:


Expected output:

4.	Verify Data


Example validation query:

 
7.	Power BI Integration

To connect Power BI Desktop to the data warehouse:
1.	Open Power BI Desktop.
2.	Click Get Data ‚Üí PostgreSQL.
3.	Use the following credentials:
 	Server: localhost
 	Port: 55432
 	Database: smart_retail_dw
 	Username: postgres
 	Password: sra_password



8.	Sample Dashboard Metrics

The project enables the creation of the following analytical views:
 	üí∞ Total Revenue KPI
 	üì¶ Total Orders
 	üìà Revenue by Month
 	üè∑ Sales by Category
 	üí≥ Payment Method Distribution
 	üî• Top 10 Products
 	üìç Sales by City



9.	ETL Process Overview

The ETL pipeline performs the following logical steps:
1.	Data ingestion from CSV / generated dataset.
2.	Data cleansing and validation.
3.	Data transformation.
4.	Star schema mapping.
5.	Bulk insert into PostgreSQL.



10.	Configuration

Environment Variables
 
Variable	Description
DB_PASSWORD
PostgreSQL password
Docker Services

Service	Port
PostgreSQL	55432
MongoDB	27017


11.	Project Objectives & Future Enhancements
Objectives
 	Demonstrate practical data engineering workflow.
 	Design analytical-ready schema.
 	Build BI-ready warehouse.
 	Simulate enterprise retail analytics architecture.

Future Enhancements
 	Add Apache Airflow for orchestration.
 	Implement Kafka streaming ingestion.
 	Add CDC with Debezium.
 	Deploy Superset for server-side BI.
 	Add ClickHouse for high-performance analytics.
 	Cloud deployment on AWS / GCP.



12.	Why This Project Matters

This project showcases real-world warehouse modeling, Dockerized infrastructure management, ETL pipeline engineering, and BI integration. It demonstrates end-to-end data lifecycle management.
It reflects skills relevant for the following roles:
 	Data Engineering
 	Analytics Engineering
 	Business Intelligence
 	Data Architecture
 
 




Author: Nicolas Salloum (Nix) | Senior Data Architect ISE Department ‚Äì Valoores SAL
¬© 2023 Smart Retail Analytics. All rights reserved.
